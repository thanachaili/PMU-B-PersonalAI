{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOHrGffiRYiw0tgr+mrwlV7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thanachaili/PMU-B-PersonalAI/blob/main/Learning_from_Biosignal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "E_06lkXBISyX",
        "outputId": "ee64cedb-6ff0-4f09-8170-d6865a2761e3"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-0b3f982b83bc>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_logger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'logger'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import sklearn.metrics as skmetrics\n",
        "import timeit\n",
        "\n",
        "from logger import get_logger\n",
        "\n",
        "\n",
        "def simple_model():\n",
        "    model = nn.Sequential(\n",
        "        nn.Conv1d(in_channels=1, out_channels=64, kernel_size=50, stride=6, bias=False),\n",
        "        nn.BatchNorm1d(num_features=64, eps=0.001, momentum=0.01),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool1d(kernel_size=8, stride=8),\n",
        "        nn.Dropout(p=0.5),\n",
        "        nn.Conv1d(in_channels=64, out_channels=64, kernel_size=8, stride=1, bias=False),\n",
        "        nn.BatchNorm1d(num_features=64, eps=0.001, momentum=0.01),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool1d(kernel_size=4, stride=4),\n",
        "        nn.Dropout(p=0.5),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features=384, out_features=5, bias=False)\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "class SimpleModel:\n",
        "\n",
        "    def __init__(self, config, device):\n",
        "        self.model = simple_model()\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters())\n",
        "        self.loss = nn.CrossEntropyLoss(reduce=False)\n",
        "        self.global_epoch = 0\n",
        "        self.global_step = 0\n",
        "        self.device = device\n",
        "        self.config = config\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def train(self, minibatch_fn):\n",
        "        start = timeit.default_timer()\n",
        "        preds, trues, losses, outputs = ([], [], [], {})\n",
        "        self.model.train()\n",
        "        for x, y, w, sl, re in minibatch_fn:\n",
        "            x = torch.from_numpy(x).to(self.device)  # shape(batch_size * seq_length, in_channels, input_length)\n",
        "            y = torch.from_numpy(y).to(self.device)  # shape(batch_size * seq_length, )\n",
        "            w = torch.from_numpy(w).to(self.device)  # shape(batch_size * seq_length, )\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            y_pred = self.model(x)\n",
        "            loss = self.loss(y_pred, y)\n",
        "            loss = torch.mul(loss, w)  # w=0 if for padded samples\n",
        "            loss = loss.sum() / w.sum()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            losses.append(loss.cpu().detach().numpy())\n",
        "            self.global_step += 1\n",
        "\n",
        "            tmp_preds = np.reshape(np.argmax(y_pred.cpu().detach().numpy(), axis=1), (self.config[\"batch_size\"], self.config[\"seq_length\"]))\n",
        "            tmp_trues = np.reshape(y.cpu().detach().numpy(), (self.config[\"batch_size\"], self.config[\"seq_length\"]))\n",
        "            for i in range(self.config[\"batch_size\"]):\n",
        "                preds.extend(tmp_preds[i, :sl[i]])\n",
        "                trues.extend(tmp_trues[i, :sl[i]])\n",
        "\n",
        "        acc = skmetrics.accuracy_score(y_true=trues, y_pred=preds)\n",
        "        all_loss = np.array(losses).mean()\n",
        "        f1_score = skmetrics.f1_score(y_true=trues, y_pred=preds, average=\"macro\")\n",
        "        cm = skmetrics.confusion_matrix(y_true=trues, y_pred=preds, labels=[0, 1, 2, 3, 4])\n",
        "        stop = timeit.default_timer()\n",
        "        duration = stop - start\n",
        "        outputs.update({\n",
        "            \"global_step\": self.global_step,\n",
        "            \"train/trues\": trues,\n",
        "            \"train/preds\": preds,\n",
        "            \"train/accuracy\": acc,\n",
        "            \"train/loss\": all_loss,\n",
        "            \"train/f1_score\": f1_score,\n",
        "            \"train/cm\": cm,\n",
        "            \"train/duration\": duration,\n",
        "\n",
        "        })\n",
        "        self.global_epoch += 1\n",
        "        return outputs\n",
        "\n",
        "    def evaluate(self, minibatch_fn):\n",
        "        start = timeit.default_timer()\n",
        "        preds, trues, losses, outputs = ([], [], [], {})\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for x, y, w, sl, re in minibatch_fn:\n",
        "                x = torch.from_numpy(x).to(self.device)  # shape(batch_size * seq_length, in_channels, input_length)\n",
        "                y = torch.from_numpy(y).to(self.device)  # shape(batch_size * seq_length, )\n",
        "                w = torch.from_numpy(w).to(self.device)  # shape(batch_size * seq_length, )\n",
        "\n",
        "                y_pred = self.model(x)\n",
        "                loss = self.loss(y_pred, y)\n",
        "                loss = torch.mul(loss, w)  # w=0 if for padded samples\n",
        "                loss = loss.sum() / w.sum()\n",
        "                losses.append(loss.cpu().detach().numpy())\n",
        "\n",
        "                tmp_preds = np.reshape(np.argmax(y_pred.cpu().detach().numpy(), axis=1), (self.config[\"batch_size\"], self.config[\"seq_length\"]))\n",
        "                tmp_trues = np.reshape(y.cpu().detach().numpy(), (self.config[\"batch_size\"], self.config[\"seq_length\"]))\n",
        "                for i in range(self.config[\"batch_size\"]):\n",
        "                    preds.extend(tmp_preds[i, :sl[i]])\n",
        "                    trues.extend(tmp_trues[i, :sl[i]])\n",
        "\n",
        "        acc = skmetrics.accuracy_score(y_true=trues, y_pred=preds)\n",
        "        all_loss = np.array(losses).mean()\n",
        "        f1_score = skmetrics.f1_score(y_true=trues, y_pred=preds, average=\"macro\")\n",
        "        cm = skmetrics.confusion_matrix(y_true=trues, y_pred=preds, labels=[0, 1, 2, 3, 4])\n",
        "        stop = timeit.default_timer()\n",
        "        duration = stop - start\n",
        "        outputs.update({\n",
        "            \"test/trues\": trues,\n",
        "            \"test/preds\": preds,\n",
        "            \"test/accuracy\": acc,\n",
        "            \"test/loss\": all_loss,\n",
        "            \"test/f1_score\": f1_score,\n",
        "            \"test/cm\": cm,\n",
        "            \"test/duration\": duration,\n",
        "\n",
        "        })\n",
        "        return outputs\n",
        "\n",
        "    def save_checkpoint(self, name, output_dir):\n",
        "        if not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "        save_path = os.path.join(output_dir, \"{}.ckpt\".format(name))\n",
        "        torch.save(self.model.state_dict(), save_path)\n",
        "        return save_path\n",
        "\n",
        "    def load_checkpoint(self, name, model_dir):\n",
        "        load_path = os.path.join(model_dir, \"{}.ckpt\".format(name))\n",
        "        self.model.load_state_dict(torch.load(load_path))\n",
        "        return load_path\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = simple_model()\n",
        "    fake_x = torch.randn(size=(2, 1, 3000))\n",
        "    print(f\"fake_x: {fake_x.shape}\")\n",
        "    y_pred = model(fake_x)\n",
        "    print(f\"y_pred: {y_pred.shape}\")\n",
        "    print('Successfully run the model')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "sIiZppa2KPgC",
        "outputId": "c814c944-3c27-43c9-efb5-61dd6bb8b747"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-98ce3e5cfe3b>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_logger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'logger'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}